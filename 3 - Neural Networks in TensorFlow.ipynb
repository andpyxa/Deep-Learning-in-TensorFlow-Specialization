{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/Logo.png?alt=media&token=06318ee3-d7a0-44a0-97ae-2c95f110e3ac\" width=\"100\" height=\"100\" align=\"right\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Neural Networks in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Artificial Intelligence, Machine Learning and Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Artificial Intelligence </font>\n",
    "> <font size=\"3\">**“Artificial Intelligence is the science and engineering of making intelligent machines, especially intelligent computer programs.” by John McCarthy, 1955**</font>\n",
    "    \n",
    "> <font size=\"3\">**In essence, AI is a <span style=\"color:#4285F4\">machine</span> with <span style=\"color:#4285F4\">cognitive functions</span> to solve problems that are usually done by humans with our <span style=\"color:#4285F4\">natural intelligence</span>**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Machine Learning </font>\n",
    "\n",
    "<font size=\"3\">**Machine learning is an application of artificial intelligence that provides machines the ability to <span style=\"color:#4285F4\">detect patterns</span> & <span style=\"color:#4285F4\">make predictions and recommendations</span>.**</font>\n",
    "\n",
    "<font size=\"3\">**Machine learning algorithms are often categorized as**</font>\n",
    "> <font size=\"3\">**Supervised learning**</font>\n",
    "\n",
    "> <font size=\"3\">**Unsupervised learning**</font>\n",
    "\n",
    "> <font size=\"3\">**Reinforcement learning**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Sup%2C%20Unsup%2C%20Rein.png?alt=media&token=4baee322-267b-4aab-b7b9-101b2c88685e\" width=\"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Neural Network </font>\n",
    "\n",
    "<font size=\"3\">**A neural network is a massively parallel distributed processor <span style=\"color:#4285F4\">(network)</span> made up of simple processing units <span style=\"color:#4285F4\">(neurons)</span>. It has a natural propensity for:**</font>\n",
    "\n",
    "> <font size=\"3\">**storing experiential knowledge through <span style=\"color:#4285F4\">learning</span>**</font>\n",
    "\n",
    "> <font size=\"3\">**making it available for use <span style=\"color:#4285F4\">(classification/prediction)</span>**</font>\n",
    "\n",
    "<font size=\"3\">**Neural network resembles the brain in two respects:**</font>\n",
    "> <font size=\"3\">**Knowledge is acquired by the <span style=\"color:#4285F4\">network</span> through a <span style=\"color:#4285F4\">learning process</span>**</font>\n",
    "\n",
    "> <font size=\"3\">**Interneuron connection strengths, known as <span style=\"color:#4285F4\">synaptic weights</span>, are used to store the acquired knowledge**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 A Gentle Introduction of Machine Learning and Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Rule-based Expert System </font>\n",
    "\n",
    "<font size=\"3\">**A rule-based expert system is the simplest form of artificial intelligence and uses prescribed knowledge-based rules to solve a problem. The objective of an expert system is to take knowledge from a human expert and convert this into a number of well defined rules to apply explicitly to the input data.**</font>\n",
    "\n",
    "<font size=\"3\">**In the most basic form, the rules are commonly conditional statements <span style=\"color:#4285F4\">(if a, then x, else if b, then y)</span>. These systems should be applied to smaller problems. It is mainly because the more complex a system is, the more rules that are required to describe it, and thus increased difficulty to define all the rules for all possible outcomes.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#176BEF'> Examples </font>\n",
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3AndGate.png?alt=media&token=0ffecb41-4b43-468f-8215-556e2fbefbdd\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AndGate(A, B):\n",
    "    if A == 0:\n",
    "        if B == 0:\n",
    "            C = 0\n",
    "        elif B == 1:\n",
    "            C = 0\n",
    "    elif A == 1:\n",
    "        if B == 0:\n",
    "            C = 0\n",
    "        elif B == 1:\n",
    "            C =1\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(AndGate(0,0))\n",
    "print(AndGate(0,1))\n",
    "print(AndGate(1,0))\n",
    "print(AndGate(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Machine Learning </font>\n",
    "\n",
    "<font size=\"3\">**Machine learning is an application of artificial intelligence that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. Machine learning focuses on the development of computer programs that can access data and use it to learn for themselves.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3MLConcept.png?alt=media&token=fbe246b6-ee85-45e5-b4a7-ba15e00043d7\" width=\"1000\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#176BEF'> Examples </font>\n",
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3MLExample.png?alt=media&token=153c3e16-5f9b-4fb9-a47e-b348b976262b\" width=\"700\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "X=np.array([[0, 0], \n",
    "            [0, 1],\n",
    "            [1, 0], \n",
    "            [1, 1]])\n",
    "\n",
    "y=np.array([0, 0, 0, 1]).reshape(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7190 - accuracy: 0.7500\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7159 - accuracy: 0.7500\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7137 - accuracy: 0.7500\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7119 - accuracy: 0.7500\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7103 - accuracy: 0.7500\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.7089 - accuracy: 0.7500\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.7075 - accuracy: 0.7500\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.7063 - accuracy: 0.7500\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.7051 - accuracy: 0.7500\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7040 - accuracy: 0.7500\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7029 - accuracy: 0.7500\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.7018 - accuracy: 0.7500\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.7008 - accuracy: 0.7500\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6998 - accuracy: 0.7500\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6988 - accuracy: 0.7500\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6979 - accuracy: 0.7500\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6969 - accuracy: 0.7500\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6960 - accuracy: 0.7500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6951 - accuracy: 0.7500\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6942 - accuracy: 0.7500\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6933 - accuracy: 0.7500\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6924 - accuracy: 0.7500\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6915 - accuracy: 0.7500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6907 - accuracy: 0.7500\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6898 - accuracy: 0.7500\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6890 - accuracy: 0.7500\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6881 - accuracy: 0.7500\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6873 - accuracy: 0.7500\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6865 - accuracy: 0.7500\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6856 - accuracy: 0.7500\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6848 - accuracy: 0.7500\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6840 - accuracy: 0.7500\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6832 - accuracy: 0.7500\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6824 - accuracy: 0.7500\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6816 - accuracy: 0.7500\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6808 - accuracy: 0.7500\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.6800 - accuracy: 0.7500\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6792 - accuracy: 0.7500\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6784 - accuracy: 0.7500\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6776 - accuracy: 0.7500\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6768 - accuracy: 0.7500\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6760 - accuracy: 0.7500\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6752 - accuracy: 0.7500\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6744 - accuracy: 0.7500\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.7500\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6728 - accuracy: 0.7500\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6721 - accuracy: 0.7500\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6713 - accuracy: 0.7500\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6705 - accuracy: 0.7500\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6697 - accuracy: 0.7500\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6689 - accuracy: 0.7500\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6681 - accuracy: 0.7500\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6673 - accuracy: 0.7500\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6665 - accuracy: 0.7500\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6657 - accuracy: 0.7500\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6650 - accuracy: 0.7500\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6642 - accuracy: 0.7500\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6634 - accuracy: 0.7500\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6626 - accuracy: 0.7500\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6618 - accuracy: 0.7500\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6610 - accuracy: 0.7500\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6602 - accuracy: 0.7500\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6594 - accuracy: 0.7500\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6586 - accuracy: 0.7500\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6578 - accuracy: 0.7500\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6570 - accuracy: 0.7500\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.6562 - accuracy: 0.7500\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6554 - accuracy: 0.7500\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6546 - accuracy: 0.7500\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6538 - accuracy: 0.7500\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6530 - accuracy: 0.7500\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6522 - accuracy: 0.7500\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6514 - accuracy: 0.7500\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6506 - accuracy: 0.7500\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.6498 - accuracy: 0.7500\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6491 - accuracy: 0.7500\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6483 - accuracy: 0.7500\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6475 - accuracy: 0.7500\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6467 - accuracy: 0.7500\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6459 - accuracy: 0.7500\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6451 - accuracy: 0.7500\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6443 - accuracy: 0.7500\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.6435 - accuracy: 0.7500\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6427 - accuracy: 0.7500\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6419 - accuracy: 0.7500\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6412 - accuracy: 0.7500\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6396 - accuracy: 0.7500\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6388 - accuracy: 0.7500\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6380 - accuracy: 0.7500\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.7500\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6364 - accuracy: 0.7500\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6356 - accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6349 - accuracy: 0.7500\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6341 - accuracy: 0.7500\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6333 - accuracy: 0.7500\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6325 - accuracy: 0.7500\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6317 - accuracy: 0.7500\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6310 - accuracy: 0.7500\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6302 - accuracy: 0.7500\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6294 - accuracy: 0.7500\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6286 - accuracy: 0.7500\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6278 - accuracy: 0.7500\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6271 - accuracy: 0.7500\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6263 - accuracy: 0.7500\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6255 - accuracy: 0.7500\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6247 - accuracy: 0.7500\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6239 - accuracy: 0.7500\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6232 - accuracy: 0.7500\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6224 - accuracy: 0.7500\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6216 - accuracy: 0.7500\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6208 - accuracy: 0.7500\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6201 - accuracy: 0.7500\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6193 - accuracy: 0.7500\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6185 - accuracy: 0.7500\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6177 - accuracy: 0.7500\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6170 - accuracy: 0.5000\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6162 - accuracy: 0.5000\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6154 - accuracy: 0.5000\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6147 - accuracy: 0.5000\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6139 - accuracy: 0.5000\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6131 - accuracy: 0.5000\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6123 - accuracy: 0.5000\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6116 - accuracy: 0.5000\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6108 - accuracy: 0.5000\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6100 - accuracy: 0.5000\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6093 - accuracy: 0.5000\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6085 - accuracy: 0.5000\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6077 - accuracy: 0.5000\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6070 - accuracy: 0.5000\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6062 - accuracy: 0.5000\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6054 - accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6047 - accuracy: 0.5000\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6039 - accuracy: 0.5000\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6031 - accuracy: 0.5000\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6024 - accuracy: 0.5000\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.6016 - accuracy: 0.5000\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6008 - accuracy: 0.5000\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.6001 - accuracy: 0.5000\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5993 - accuracy: 0.5000\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5985 - accuracy: 0.5000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5978 - accuracy: 0.5000\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5970 - accuracy: 0.5000\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5963 - accuracy: 0.5000\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5955 - accuracy: 0.5000\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.5000\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5940 - accuracy: 0.5000\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5932 - accuracy: 0.5000\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5925 - accuracy: 0.5000\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5917 - accuracy: 0.5000\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5909 - accuracy: 0.5000\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5902 - accuracy: 0.5000\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5894 - accuracy: 0.5000\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5886 - accuracy: 0.5000\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5879 - accuracy: 0.5000\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5871 - accuracy: 0.5000\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.5000\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5856 - accuracy: 0.5000\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5848 - accuracy: 0.5000\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5841 - accuracy: 0.5000\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5833 - accuracy: 0.5000\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5825 - accuracy: 0.5000\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5817 - accuracy: 0.5000\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.5810 - accuracy: 0.5000\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5802 - accuracy: 0.5000\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5794 - accuracy: 0.5000\n",
      "Epoch 167/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5786 - accuracy: 0.5000\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5779 - accuracy: 0.5000\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 996us/step - loss: 0.5771 - accuracy: 0.5000\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5763 - accuracy: 0.5000\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5755 - accuracy: 0.5000\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5748 - accuracy: 0.5000\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5740 - accuracy: 0.5000\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5732 - accuracy: 0.5000\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5724 - accuracy: 0.5000\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5716 - accuracy: 0.5000\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5709 - accuracy: 0.5000\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5701 - accuracy: 0.5000\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5693 - accuracy: 0.5000\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 919us/step - loss: 0.5685 - accuracy: 0.5000\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5677 - accuracy: 0.5000\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5669 - accuracy: 0.5000\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5661 - accuracy: 0.5000\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5654 - accuracy: 0.5000\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5646 - accuracy: 0.5000\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.5638 - accuracy: 0.5000\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5630 - accuracy: 0.5000\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5622 - accuracy: 0.5000\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5614 - accuracy: 0.5000\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5606 - accuracy: 0.5000\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5598 - accuracy: 0.5000\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5590 - accuracy: 0.5000\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.5582 - accuracy: 0.7500\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5575 - accuracy: 0.7500\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5567 - accuracy: 0.7500\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5559 - accuracy: 0.7500\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.5551 - accuracy: 0.7500\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.5543 - accuracy: 0.7500\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5535 - accuracy: 0.7500\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5527 - accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1a5f92940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape=(X.shape[1],)))\n",
    "model.add(Dense(1, activation ='sigmoid'))\n",
    "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 75.0 %\n"
     ]
    }
   ],
   "source": [
    "Loss, Acc = model.evaluate(X, y, verbose=0)\n",
    "print('The accuracy is:', Acc*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Machine Learning Requires Big Data </font>\n",
    "\n",
    "<font size=\"3\">**The process of learning begins with observations or data, such as examples, direct experience, or instruction, in order to look for patterns in data and make future prediction or classification based on the examples that it learnt. Therefore, machine learning algorithms become more effective and accurate as the size of training datasets grows, and require big data to work. Without large, well maintained datasets, machine learning algorithms fall far short of their potential.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#176BEF'> Examples </font>\n",
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array([[0, 0], \n",
    "            [0, 1],\n",
    "            [1, 0], \n",
    "            [1, 1]])\n",
    "\n",
    "y=np.array([0, 0, 0, 1]).reshape(4,1)\n",
    "\n",
    "X_new = np.repeat(X, 50, axis=0)\n",
    "y_new = np.repeat(y, 50, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/7 [===>..........................] - ETA: 0s - loss: 0.7119 - accuracy: 0.7188WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0010s vs `on_train_batch_end` time: 0.0020s). Check your callbacks.\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6964 - accuracy: 0.7500\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6919 - accuracy: 0.7500\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.7500\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6857 - accuracy: 0.7500\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6829 - accuracy: 0.7500\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6801 - accuracy: 0.7500\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6773 - accuracy: 0.7500\n",
      "Epoch 8/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6745 - accuracy: 0.7500\n",
      "Epoch 9/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6719 - accuracy: 0.7500\n",
      "Epoch 10/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6689 - accuracy: 0.7500\n",
      "Epoch 11/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6660 - accuracy: 0.7500\n",
      "Epoch 12/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6632 - accuracy: 0.7500\n",
      "Epoch 13/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6606 - accuracy: 0.7500\n",
      "Epoch 14/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6578 - accuracy: 0.7500\n",
      "Epoch 15/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6551 - accuracy: 0.7500\n",
      "Epoch 16/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6525 - accuracy: 0.7500\n",
      "Epoch 17/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6497 - accuracy: 0.7500\n",
      "Epoch 18/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6470 - accuracy: 0.7500\n",
      "Epoch 19/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6440 - accuracy: 0.7500\n",
      "Epoch 20/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6411 - accuracy: 0.7500\n",
      "Epoch 21/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6381 - accuracy: 0.7500\n",
      "Epoch 22/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6350 - accuracy: 0.7500\n",
      "Epoch 23/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6317 - accuracy: 0.7500\n",
      "Epoch 24/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6285 - accuracy: 0.7500\n",
      "Epoch 25/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6253 - accuracy: 0.7500\n",
      "Epoch 26/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.6220 - accuracy: 0.7500\n",
      "Epoch 27/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6188 - accuracy: 0.7500\n",
      "Epoch 28/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.6154 - accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6120 - accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6086 - accuracy: 0.7500\n",
      "Epoch 31/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6051 - accuracy: 0.7500\n",
      "Epoch 32/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6017 - accuracy: 0.7500\n",
      "Epoch 33/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5981 - accuracy: 0.7500\n",
      "Epoch 34/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5945 - accuracy: 0.7500\n",
      "Epoch 35/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5908 - accuracy: 0.7500\n",
      "Epoch 36/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5875 - accuracy: 0.7500\n",
      "Epoch 37/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5840 - accuracy: 0.7500\n",
      "Epoch 38/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5805 - accuracy: 0.7500\n",
      "Epoch 39/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5770 - accuracy: 0.7500\n",
      "Epoch 40/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5733 - accuracy: 0.7500\n",
      "Epoch 41/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5695 - accuracy: 0.7500\n",
      "Epoch 42/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5657 - accuracy: 0.7500\n",
      "Epoch 43/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5619 - accuracy: 0.7500\n",
      "Epoch 44/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5583 - accuracy: 0.7500\n",
      "Epoch 45/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5548 - accuracy: 0.7500\n",
      "Epoch 46/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.5517 - accuracy: 0.7500\n",
      "Epoch 47/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5482 - accuracy: 0.7500\n",
      "Epoch 48/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5446 - accuracy: 0.7500\n",
      "Epoch 49/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5271 - accuracy: 0.78 - 0s 3ms/step - loss: 0.5406 - accuracy: 0.7500\n",
      "Epoch 50/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5367 - accuracy: 0.7500\n",
      "Epoch 51/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 0.5139 - accuracy: 0.81 - 0s 4ms/step - loss: 0.5330 - accuracy: 0.7500\n",
      "Epoch 52/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.5296 - accuracy: 0.7500\n",
      "Epoch 53/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5263 - accuracy: 0.7500\n",
      "Epoch 54/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5228 - accuracy: 0.7500\n",
      "Epoch 55/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5193 - accuracy: 0.7500\n",
      "Epoch 56/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.5156 - accuracy: 0.7500\n",
      "Epoch 57/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5118 - accuracy: 0.7500\n",
      "Epoch 58/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.5079 - accuracy: 0.7500\n",
      "Epoch 59/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.5038 - accuracy: 0.7500\n",
      "Epoch 60/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.7500\n",
      "Epoch 61/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4959 - accuracy: 0.7500\n",
      "Epoch 62/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4920 - accuracy: 0.7500\n",
      "Epoch 63/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.7500\n",
      "Epoch 64/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4840 - accuracy: 0.7500\n",
      "Epoch 65/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4800 - accuracy: 0.7500\n",
      "Epoch 66/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4762 - accuracy: 0.7500\n",
      "Epoch 67/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4723 - accuracy: 0.7500\n",
      "Epoch 68/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.7500\n",
      "Epoch 69/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4648 - accuracy: 0.7500\n",
      "Epoch 70/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4609 - accuracy: 0.7500\n",
      "Epoch 71/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4571 - accuracy: 0.7500\n",
      "Epoch 72/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4533 - accuracy: 0.7500\n",
      "Epoch 73/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4496 - accuracy: 0.7500\n",
      "Epoch 74/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4458 - accuracy: 0.7500\n",
      "Epoch 75/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4418 - accuracy: 0.7500\n",
      "Epoch 76/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4382 - accuracy: 0.7500\n",
      "Epoch 77/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4343 - accuracy: 0.7500\n",
      "Epoch 78/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4308 - accuracy: 0.7500\n",
      "Epoch 79/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4270 - accuracy: 0.7500\n",
      "Epoch 80/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4231 - accuracy: 0.7500\n",
      "Epoch 81/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.7500\n",
      "Epoch 82/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4160 - accuracy: 0.7500\n",
      "Epoch 83/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4123 - accuracy: 0.7500\n",
      "Epoch 84/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.4085 - accuracy: 0.7500\n",
      "Epoch 85/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.4047 - accuracy: 0.9150\n",
      "Epoch 86/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.4012 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "7/7 [==============================] - 0s 6ms/step - loss: 0.3977 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3942 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3906 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3869 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3833 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3793 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3755 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3719 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3684 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.3648 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3611 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "7/7 [==============================] - 0s 4ms/step - loss: 0.3572 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.3534 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "7/7 [==============================] - 0s 5ms/step - loss: 0.3497 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b1a64f1610>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(4, input_shape=(X.shape[1],)))\n",
    "model.add(Dense(1, activation ='sigmoid'))\n",
    "model.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_new, y_new, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "Loss_new, Acc_new = model.evaluate(X_new, y_new, verbose=0)\n",
    "print('The accuracy is:', Acc_new*100, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"background-color:#EA4335; color:white\">&nbsp;!&nbsp;</span></font>\n",
    "<font size=\"3\">**With large enough data, not only the accuracy is higher, but also the convergence rate is faster.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Logistic regression is a simple form of a neural network:**</font>\n",
    "\n",
    "> <font size=\"3\">**Contain only <span style=\"color:#4285F4\">one layer neural network</span>**</font>\n",
    "\n",
    "> <font size=\"3\">**<span style=\"color:#4285F4\">Classifies</span> data categorically (e.g. 0 or 1)**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**There are two main steps in logistic regression:**</font>\n",
    "\n",
    "<font size=\"3\">**1. Parameters Initialization**</font>\n",
    "> <font size=\"3\">**Weights <span style=\"color:#4285F4\">w</span>**</font> <br>\n",
    "> <font size=\"3\">**Biases <span style=\"color:#4285F4\">b</span>**</font> <br>\n",
    "\n",
    "<font size=\"3\">**2. Loop:**</font> <br>\n",
    "> <font size=\"3\">**1. Forward propagation - Calculate <span style=\"color:#4285F4\">Loss</span>**</font> <br>\n",
    "> <font size=\"3\">**2. Backward propagation - Calculate <span style=\"color:#4285F4\">Gradient</span>**</font> <br>\n",
    "> <font size=\"3\">**3. Gradient descent - <span style=\"color:#4285F4\">Update parameters</span>**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #34A853\"> </hr>\n",
    "\n",
    "### <font color='#34A853'> Logistic Regression - Forward Propagation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**1. Takes input and calculate <span style=\"color:#4285F4\">weighted sum</span>**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic1.png?alt=media&token=9be673c5-9f47-49f3-a13e-278bb0731053\" width=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**2. Passes the weighted sum through <span style=\"color:#4285F4\">sigmoid function</span>**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic2.png?alt=media&token=41b063cd-8f50-486e-a7ba-49f83a3baab9\" width=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**3. Returns an output of <span style=\"color:#4285F4\">probability</span> between 0 & 1**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic3.png?alt=media&token=ff57a33f-7d76-4858-8a73-f3302f66e6de\" width=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**4. Calculate <span style=\"color:#4285F4\">loss</span>**</font> <br>\n",
    "> <font size=\"3\"><img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic4.png?alt=media&token=99feb6f4-3947-4f4a-ae80-66060ee1b09a\" width=\"500\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic5.png?alt=media&token=26dccc8a-76e1-49b2-b118-181186c0a7c3\" width=\"550\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#34A853'> Logistic Regression - Backward Propagation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**1. <span style=\"color:#4285F4\">Loss function</span> represent one training sample**</font> <br>\n",
    "\n",
    "> <font size=\"3\">**2. <span style=\"color:#4285F4\">Cost function</span>, <span style=\"color:#4285F4\">J</span>, for all training samples**</font> <br>\n",
    "> <font size=\"3\"><img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic6.png?alt=media&token=2a7f2cde-c88d-41fa-a5cc-a2f3a016c4ff\" width=\"350\" align=\"center\"/>\n",
    "    \n",
    "> <font size=\"3\">**3. Calculate <span style=\"color:#4285F4\">gradient</span> w.r.t weights and bias**</font>\n",
    "> <font size=\"3\"><img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic7.png?alt=media&token=71fe0f8d-a0c2-49f4-9175-581acc20f530\" width=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#34A853'> Logistic Regression - Gradient Descent </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**1. Set <span style=\"color:#4285F4\">learning rate</span>, <span style=\"color:#4285F4\">𝜸</span>**</font> <br>\n",
    "\n",
    "> <font size=\"3\">**2. Update parameters <span style=\"color:#4285F4\">weights</span> and <span style=\"color:#4285F4\">bias</span>**</font> <br>\n",
    "> <font size=\"3\"><img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic8.png?alt=media&token=942b04f4-ed1d-4d19-bc7e-1e3850344bfe\" width=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Logistic Regression - Full Training Algorithm </font>\n",
    "\n",
    "<font size=\"3\">**For loop (<span style=\"color:#4285F4\">epochs</span>):**</font> <br>\n",
    "> <font size=\"3\">**1. Forward propagation – Calculate <span style=\"color:#4285F4\">Loss</span>**</font> <br>\n",
    "> <font size=\"3\">**2. Backward propagation – Calculate <span style=\"color:#4285F4\">Gradient</span>**</font> <br>\n",
    "> <font size=\"3\">**3. Gradient descent – Update <span style=\"color:#4285F4\">parameters</span>**</font>\n",
    "\n",
    "    \n",
    "<hr style=\"border:2px solid #34A853\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 From Logistic Regression to Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Logistic regression is a simple form of a neural network with <span style=\"color:#4285F4\">one layer </span> and <span style=\"color:#4285F4\">classifies</span> data categorically (e.g. 0 or 1)**</font>\n",
    "\n",
    "<font size=\"3\">**Neural network can be much complex in reality. But for understanding the concept, neural network can be considered to be formed by stacking together a lot of little sigmoid units.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN1.png?alt=media&token=dd89a015-37c0-4849-a639-3e7bedeed65c\" width=\"650\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Similar to logistic regression, there are two main steps in neural network:**</font>\n",
    "\n",
    "<font size=\"3\">**1. Parameters Initialization**</font>\n",
    "> <font size=\"3\">**Weights <span style=\"color:#4285F4\">w</span>**</font> <br>\n",
    "> <font size=\"3\">**Biases <span style=\"color:#4285F4\">b</span>**</font> <br>\n",
    "\n",
    "<font size=\"3\">**2. For loop (<span style=\"color:#4285F4\">epochs</span>):**</font> <br>\n",
    "> <font size=\"3\">**1. Forward propagation - Calculate <span style=\"color:#4285F4\">Loss</span>**</font> <br>\n",
    "> <font size=\"3\">**2. Backward propagation - Calculate <span style=\"color:#4285F4\">Gradient</span>**</font> <br>\n",
    "> <font size=\"3\">**3. Gradient descent - <span style=\"color:#4285F4\">Update parameters</span>**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #34A853\"> </hr>\n",
    "\n",
    "### <font color='#34A853'> Neural Network - Forward Propagation </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**STEP 1 - <font color='Red'>Hidden Layer 1 </font> <font color='#7F00FF'>Node 1</font>**</font>\n",
    ">> <font size=\"3\">**1. Takes input and calculate <span style=\"color:#4285F4\">weighted sum</span>**</font> <br>\n",
    ">> <font size=\"3\">**2. Passes the weighted sum through <span style=\"color:#4285F4\">sigmoid function</span>**</font>\n",
    "\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN2.png?alt=media&token=b8f4c510-9e58-4cd9-a22e-b784142c94b3\" width=\"850\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"background-color:#EA4335; color:white\">&nbsp;!&nbsp;</span></font>\n",
    "<font size=\"3\">**Notation: Z<sub>1</sub><sup>[1]</sup>, where**</font>\n",
    "> <font size=\"3\">**[1] represents the corresponding <font color='Red'>layer</font>**</font> <br>\n",
    "> <font size=\"3\">**1 represents the corresponding <font color='#7F00FF'>node</font>**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**STEP 1 - <font color='Red'>Hidden Layer 1 </font> <font color='#7F00FF'>Node 2</font>**</font>\n",
    ">> <font size=\"3\">**1. Takes input and calculate <span style=\"color:#4285F4\">weighted sum</span>**</font> <br>\n",
    ">> <font size=\"3\">**2. Passes the weighted sum through <span style=\"color:#4285F4\">sigmoid function</span>**</font>\n",
    "\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN3.png?alt=media&token=20212b88-b355-4450-ab76-725cbb59d4a4\" width=\"850\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"background-color:#EA4335; color:white\">&nbsp;!&nbsp;</span></font>\n",
    "<font size=\"3\">**z, w, b, a are all vectors and sigmoid function is applied element-wise to z. Therefore, vectorization can be applied to avoid any for loop to improve the efficiency.**</font>\n",
    "\n",
    "### <font color='Orange'> Vectorization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN4.png?alt=media&token=96cfb2b4-ad8c-4f27-8621-8d0120dc90b2\" width=\"850\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"background-color:#EA4335; color:white\">&nbsp;!&nbsp;</span></font>\n",
    "<font size=\"3\">**Notation: All the vectors and matrix are combined and subscripts are removed to represent them as their corresponding vectors and matrix.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN5.png?alt=media&token=7d616726-cb81-4ed7-b67c-1d82bfc423ec\" width=\"850\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**STEP 1 - <font color='Red'>Hidden Layer 2 </font> <font color='#7F00FF'>Node 1</font>**</font>\n",
    ">> <font size=\"3\">**1. Takes input and calculate <span style=\"color:#4285F4\">weighted sum</span>**</font> <br>\n",
    ">> <font size=\"3\">**2. Passes the weighted sum through <span style=\"color:#4285F4\">sigmoid function</span>**</font>\n",
    "\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN6.png?alt=media&token=fb1ee7b7-5841-42dd-bd49-14baf9312e1e\" width=\"850\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> <font size=\"3\">**3. Returns an output of <span style=\"color:#4285F4\">probability</span> between 0 & 1**</font><br>\n",
    ">> <font size=\"3\">**4. Calculate <span style=\"color:#4285F4\">loss</span> and <span style=\"color:#4285F4\">cost</span>**</font> <br>\n",
    "\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN7.png?alt=media&token=2767cccb-de9f-45ab-b3cb-72535b9768a5\" width=\"850\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#34A853'> Neural Network - Backward Propagation and Gradient Descent </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**STEP 2**</font>\n",
    ">> <font size=\"3\">**1. Calculate <span style=\"color:#4285F4\">gradient</span> w.r.t weights and bias**</font>\n",
    ">> <font size=\"3\"><img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic7.png?alt=media&token=71fe0f8d-a0c2-49f4-9175-581acc20f530\" width=\"350\" align=\"center\"/>\n",
    ">> <font size=\"3\">**2. Set <span style=\"color:#4285F4\">learning rate</span>, <span style=\"color:#4285F4\">𝜸</span>**</font> <br>\n",
    ">> <font size=\"3\">**3. Update parameters <span style=\"color:#4285F4\">weights</span> and <span style=\"color:#4285F4\">bias</span>**</font> <br>\n",
    ">> <font size=\"3\"><img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3Logistic8.png?alt=media&token=942b04f4-ed1d-4d19-bc7e-1e3850344bfe\" width=\"350\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> Neural Network - Full Training Algorithm </font>\n",
    "\n",
    "<font size=\"3\">**For loop (<span style=\"color:#4285F4\">epochs</span>):**</font> <br>\n",
    "> <font size=\"3\">**1. Forward propagation – Calculate <span style=\"color:#4285F4\">Loss</span>**</font> <br>\n",
    "> <font size=\"3\">**2. Backward propagation – Calculate <span style=\"color:#4285F4\">Gradient</span>**</font> <br>\n",
    "> <font size=\"3\">**3. Gradient descent – Update <span style=\"color:#4285F4\">parameters</span>**</font>\n",
    "\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN8.png?alt=media&token=6c532cd1-8a5c-4395-884f-812026603f15\" width=\"850\" align=\"center\"/>\n",
    "\n",
    "<hr style=\"border:2px solid #34A853\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Build your first Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/1Keras.png?alt=media&token=9f4add09-14d3-49ed-bc11-f0497f6e96f1\" width=\"200\" height=\"200\" align=\"right\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**Keras is a simple tool for constructing a neural network. It is a high-level API of TensorFlow 2:**</font> \n",
    "\n",
    "> <font size=\"3\">**an approachable, highly-productive interface for solving machine learning problems, with a focus on modern deep learning.**</font>\n",
    "\n",
    "<font size=\"3\">**The core data structures of Keras are layers and models.**</font>\n",
    "\n",
    "> <font size=\"3\">**The simplest type of model is the <span style=\"color:#4285F4\">Sequential model</span>, a linear stack of layers.**</font>\n",
    "\n",
    "> <font size=\"3\">**For more complex architectures, the Keras <span style=\"color:#4285F4\">Functional API</span> should be used, which allows to build arbitrary graphs of layers, or write models entirely from scratch.**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'>*Sequential model - When to use*</font>\n",
    "\n",
    "<font size=\"3\">**A Sequential model is appropriate for**</font> \n",
    "> <font size=\"3\">**<span style=\"color:#4285F4\">a plain stack of layers</span> where each layer has <span style=\"color:#4285F4\">exactly one input tensor and one output tensor</span>.**</font> \n",
    "\n",
    "<font size=\"3\">**This is not appropriate when:**</font> \n",
    "\n",
    "> <font size=\"3\">**Your model has <span style=\"color:#4285F4\">multiple inputs or multiple outputs</span>**</font> <br>\n",
    "> <font size=\"3\">**Any of your layers has <span style=\"color:#4285F4\">multiple inputs or multiple outputs</span>**</font> <br>\n",
    "> <font size=\"3\">**You need to do <span style=\"color:#4285F4\">layer sharing</span>**</font><br>\n",
    "> <font size=\"3\">**You want <span style=\"color:#4285F4\">non-linear topology</span> (e.g. a residual connection, a multi-branch model)**</font>\n",
    "\n",
    "Reference: https://keras.io/guides/sequential_model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'>*Sequential model - How to use*</font>\n",
    "\n",
    "<font size=\"3\">**You can create a <span style=\"color:#4285F4\">Sequential model</span> by**</font> \n",
    "> <font size=\"3\">**Passing a list of layers to a Sequential constructor**</font> \n",
    "\n",
    "> <font size=\"3\">**<span style=\"background-color: #ECECEC; color:#0047bb\">.add()</span> method to incrementally setup layers**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#176BEF'> Examples </font>\n",
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [\n",
    "        Dense(2, input_shape=(3,)),\n",
    "        Dense(1, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 2)                 8         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 11\n",
      "Trainable params: 11\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape=(3,)))\n",
    "model.add(Dense(1, activation ='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #E1F6FF\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'>*Output Shape*</font>\n",
    "> <font size=\"3\">**In the output shape of the layers, the model expects the input to have a batch size as the outermost (left most) dimension. Therefore, <span style=\"color:#4285F4\">null</span> values is assigned for greater flexibility.**</font><br>\n",
    "\n",
    "> <font size=\"3\">**The second parameter of output shape simply equals to the number of neurons in the same layer.**</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'>*Parameters*</font>\n",
    "> <font size=\"3\">**<span style=\"color:#4285F4\">Dense Layer</span>: Param = (Input Size + 1) x Number of Neurons**</font><br>\n",
    "\n",
    "> <font size=\"3\">**+1 is because of Biases <span style=\"color:#4285F4\">b</span>**</font><br>\n",
    ">> <font size=\"3\">**First Dense Layer: Param = (3 + 1) x 2 = 8**</font><br>\n",
    ">> <font size=\"3\">**Second Dense Layer: Param = (2 + 1) x 1 = 3**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'>*Best Practice for Deep Learning*</font>\n",
    "<font size=\"3\">**When building a new Sequential model, it is useful to**</font> \n",
    "\n",
    "> <font size=\"3\">**1. incrementally stack layers with <span style=\"background-color: #ECECEC; color:#0047bb\">.add()</span>**</font> \n",
    "\n",
    "> <font size=\"3\">**2. frequently print model summaries with <span style=\"background-color: #ECECEC; color:#0047bb\">.summary()</span>**</font> \n",
    "\n",
    "<font size=\"3\">**This enables you to monitor how the stack of layers are connected, which is especially useful for deep network architecture.**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border:2px solid #34A853\"> </hr>\n",
    "\n",
    "### <font color='#34A853'> 6 lines of code builds a neural network  </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**1. Import <span style=\"color:#4285F4\">Sequential model</span> from TensorFlow Keras**</font>\n",
    "\n",
    "<font size=\"3\">**2. Import <span style=\"color:#4285F4\">Dense Layer</span> from TensorFlow Keras**</font>\n",
    "\n",
    "<font size=\"3\">**3. Create <span style=\"color:#4285F4\">Sequential model</span> object**</font>\n",
    "\n",
    "<font size=\"3\">**4. Add <span style=\"color:#4285F4\">1<sup>st</sup></span> layer with <span style=\"color:#4285F4\">2</span> neurons and <span style=\"color:#4285F4\">3</span> features as input**</font>\n",
    "\n",
    "<font size=\"3\">**5. Add <span style=\"color:#4285F4\">2<sup>nd</sup></span> layer with <span style=\"color:#4285F4\">1</span> neuron as output and <span style=\"color:#4285F4\">sigmoid</span> as activation function**</font>\n",
    "\n",
    "<font size=\"3\">**6. Compile the neural network model with <span style=\"color:#4285F4\">optimizer, loss function, evaluation metrics</span>**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(2, input_shape = (3,)))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'RMSprop', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN9.png?alt=media&token=664be587-f0fe-43ec-8217-5ca7779ca0dd\" width=\"350\" align=\"center\"/>\n",
    "\n",
    "<hr style=\"border:2px solid #34A853\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> 3. Create Sequential model object </font>\n",
    "> <font size=\"3\">**Keras provides two ways for constructing neural network, i.e. <span style=\"color:#4285F4\">Sequential model</span> and <span style=\"color:#4285F4\">Functional API</span>.**</font>\n",
    "\n",
    "> <font size=\"3\">**In this step, an <span style=\"color:#4285F4\">empty</span> Sequential model object is created.**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> 4. Add 1<sup>st</sup> layer with 2 neurons and 3 features as input </font>\n",
    "### <font color='Orange'> 5. Add 2<sup>nd</sup> layer with 1 neuron as output and sigmoid as activation function </font>\n",
    "> <font size=\"3\">**Once an <span style=\"color:#4285F4\">empty</span> Sequential model object is created, layers can be added via <span style=\"background-color: #ECECEC; color:#0047bb\">.add()</span> function.**</font>\n",
    "\n",
    "> <font size=\"3\">**Keras provides plenty of pre-built layers for different neural network architectures.**</font>\n",
    ">> <font size=\"3\">**Core layer:** The <span style=\"color:#4285F4\">dense layer</span> is one of the core layers. It is a standard neural network layer. It is helpful to produce output in the desired form.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Convolution layer:** This layer creates a convolution kernel. It is convolved over a single input to produce a tensor of outputs.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Embedding layer:** This layer is used as the first layer of neural network model to turn positive integers into dense vectors of fixed sizes.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Merge layer:** This layer helps merge a list of inputs. It provides many functions to make tasks easy. These functions are: Add(), subtract(), multiply(), average(), maximum(), minimum(), etc.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Dropout layer:** Dropout can be implemented by added Dropout layers into network architecture. It will help dropping-out based on user-defined hyperparameters</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Pooling layer:** It is a new layer added for the convolution layer. It helps to implement pooling operations. This layer can be added to a CNN between the layers and is useful for max-pooling operations on temporal data.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Noise layer:** This layer helps add external noise to model.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Normalization layer:** This layer helps transfer the input to a standardized form. This layer will have a mean of zero and a standard deviation of one. Keras supports normalization via the BatchNormalization layer.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Recurrent layer:** These layers are present for abstract batch class. There are two parameters: return_state, and return_sequences.</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**Locally-connected layer:** It works similarly to convolutional layer. Except, this layer does not share weights.</font>\n",
    "\n",
    "Reference: https://techvidvan.com/tutorials/keras-layers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <font size=\"3\">**Dense Layers take different parameters. Here are few commonly used parameters:**</font>\n",
    ">> <font size=\"3\">**1<sup>st</sup> parameter: <span style=\"color:#4285F4\">Number of neurons</span>**</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**2<sup>nd</sup> parameter: <span style=\"color:#4285F4\">Activation</span>**</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**3<sup>rd</sup> parameter (Only if it is a first layer): <span style=\"color:#4285F4\">Input shape</span>**</font><br>\n",
    ">>> <font size=\"3\">**If the Dense Layer is the first layer, model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model needs to receive additional parameter about its input shape.**</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**There are several possible ways to do this, e.g. passing an <span style=\"background-color: #ECECEC; color:#0047bb\">input_dim</span> or <span style=\"background-color: #ECECEC; color:#0047bb\">input_shape</span>.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='Orange'> 6. Compile the neural network model with optimizer, loss function, evaluation metrics </font>\n",
    "> <font size=\"3\">**Once the neural network architecture is setup and added into the Sequential model object, the model can be compiled with the use of <span style=\"background-color: #ECECEC; color:#0047bb\">.compile()</span> function.**</font>\n",
    "\n",
    "> <font size=\"3\">**<span style=\"background-color: #ECECEC; color:#0047bb\">.compile()</span> allows for different parameters. The most important parameters are:**</font>\n",
    "\n",
    ">> <font size=\"3\">**1<sup>st</sup> parameter: <span style=\"color:#4285F4\">Optimizer</span>**</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**2<sup>nd</sup> parameter: <span style=\"color:#4285F4\">Loss function</span>**</font><br>\n",
    "<br>\n",
    ">> <font size=\"3\">**3<sup>rd</sup> parameter: <span style=\"color:#4285F4\">Metrics</span>**</font>\n",
    "\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN10.png?alt=media&token=9223446e-8108-4082-b9c9-225018f9f54e\" width=\"550\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"3\">**<span style=\"color:#4285F4\">Loss Function</span>**</font>\n",
    "> <font size=\"3\">**Once the neural network architecture is setup and added into the Sequential model object, samples are <span style=\"color:#4285F4\">foward propagated</span> and the corresponding estimates, $\\hat{y}$ are calculated.**</font><br>\n",
    "\n",
    "> <font size=\"3\">**<span style=\"color:#4285F4\">Loss function</span> is then applied to estimate the <span style=\"color:#4285F4\">loss values</span> between the true values (i.e. Labels, y) and predicted values (i.e. Estimates, $\\hat{y}$).**</font>\n",
    "\n",
    "<font size=\"3\">**<span style=\"color:#4285F4\">Optimizer</span>**</font>\n",
    "> <font size=\"3\">**Based on the <span style=\"color:#4285F4\">loss values</span>, <span style=\"color:#4285F4\">optimizer backward propagates</span> and calculates the <span style=\"color:#4285F4\">gradients</span> w.r.t weights, W and bias, b.**</font>\n",
    "\n",
    "<font size=\"5\"><span style=\"background-color:#EA4335; color:white\">&nbsp;!&nbsp;</span></font> \n",
    "<font size=\"3\">**The training will be stopped either when:**</font>\n",
    "> <font size=\"3\">**The maximum number of epochs in <span style=\"background-color: #ECECEC; color:#0047bb\">.fit()</span> function is reached; OR**</font>\n",
    "\n",
    "> <font size=\"3\">**A monitored quantity <span style=\"background-color: #ECECEC; color:#0047bb\">.EarlyStopping()</span> function has stopped improving.**</font>\n",
    "\n",
    "<font size=\"3\">**<span style=\"color:#4285F4\">Metrics</span>**</font>\n",
    "> <font size=\"3\">**A metric is an addition evaluation function that is used to judge the performance of the model**</font>\n",
    "\n",
    "> <font size=\"3\">**Metric functions are similar to loss functions, except that the results from evaluating a metric are not used when training the model. Therefore, any loss function can also be used as a metric**</font>\n",
    "\n",
    "> <font size=\"3\">**The main reason is because it is difficult to judge the performance based on loss values, such as mean squared error (MSE) and root mean squared error (RMSE). Therefore, sometimes, an extra metric, such as accuracy and mean absolute error (MAE), is used for additional evaluation.**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"5\"><span style=\"background-color:#EA4335; color:white\">&nbsp;!&nbsp;</span></font> \n",
    "<font size=\"3\">**Combinations of output layer activation and loss functions**</font>\n",
    "<br>\n",
    "<img src=\"https://firebasestorage.googleapis.com/v0/b/deep-learning-crash-course.appspot.com/o/3NN11.png?alt=media&token=9d57d341-c9ad-4126-918e-526bde571a1b\" width=\"950\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
